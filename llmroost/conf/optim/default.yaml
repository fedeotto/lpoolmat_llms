optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.Adam
  lr: 3.0e-4
  # betas: [ 0.9, 0.999 ]
  # eps: 1e-08
  # weight_decay: 0
use_lr_scheduler: true
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  factor: 0.05
  patience: 30
  min_lr: 1.0e-6